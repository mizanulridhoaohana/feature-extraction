{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Feature Extraction and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from skimage import feature\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import io, color, img_as_ubyte\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/CORROSION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(class_output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Iterate through the images in the class folder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(class_dir):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Adjust the file extension as needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         input_image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(class_dir, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/CORROSION'"
     ]
    }
   ],
   "source": [
    "# Path directory containing the \"CORROSION\" and \"NOCORROSION\" folders\n",
    "root_dir = './data'\n",
    "output_dir = './resize_data'\n",
    "\n",
    "# Define the target size\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Function to resize and add padding to an image\n",
    "def resize_and_add_padding(image_path, output_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = ImageOps.fit(image, target_size, method=0, bleed=0.0, centering=(0.5, 0.5))\n",
    "    image.save(output_path)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the \"CORROSION\" and \"NOCORROSION\" folders\n",
    "for class_folder in ['CORROSION', 'NOCORROSION']:\n",
    "    class_dir = os.path.join(root_dir, class_folder)\n",
    "\n",
    "    # Create a subdirectory in the output directory for each class\n",
    "    class_output_dir = os.path.join(output_dir, class_folder)\n",
    "    os.makedirs(class_output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the images in the class folder\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith('.jpg'):  # Adjust the file extension as needed\n",
    "            input_image_path = os.path.join(class_dir, filename)\n",
    "            output_image_path = os.path.join(class_output_dir, filename)\n",
    "            resize_and_add_padding(input_image_path, output_image_path)\n",
    "            print(f\"Resized: {input_image_path} -> {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gray Level Co-Occurance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glcm_features(image_path):\n",
    "    img = io.imread(image_path)\n",
    "    gray = color.rgb2gray(img)\n",
    "    image = img_as_ubyte(gray)\n",
    "    \n",
    "    bins = np.array([0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 255])  # 16-bit\n",
    "    inds = np.digitize(image, bins)\n",
    "\n",
    "    max_value = inds.max() + 1\n",
    "    matrix_cooccurrence = graycomatrix(inds, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=max_value, normed=False, symmetric=False)\n",
    "\n",
    "    contrast = graycoprops(matrix_cooccurrence, 'contrast')\n",
    "    dissimilarity = graycoprops(matrix_cooccurrence, 'dissimilarity')\n",
    "    homogeneity = graycoprops(matrix_cooccurrence, 'homogeneity')\n",
    "    energy = graycoprops(matrix_cooccurrence, 'energy')\n",
    "    correlation = graycoprops(matrix_cooccurrence, 'correlation')\n",
    "    asm = graycoprops(matrix_cooccurrence, 'ASM')\n",
    "\n",
    "    return {\n",
    "        \"Contrast\": contrast,\n",
    "        \"Dissimilarity\": dissimilarity,\n",
    "        \"Homogeneity\": homogeneity,\n",
    "        \"Energy\": energy,\n",
    "        \"Correlation\": correlation,\n",
    "        \"ASM\": asm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrosion_dir = \"resize_data/CORROSION\"\n",
    "nocorrosion_dir = \"resize_data/NOCORROSION\"\n",
    "\n",
    "# Function to compute GLCM features for a directory of images\n",
    "def compute_glcm_features_for_directory(directory, label):\n",
    "    features_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            features = compute_glcm_features(image_path)\n",
    "            features[\"Label\"] = label  # Add the label\n",
    "            features_list.append(features)\n",
    "    return features_list\n",
    "\n",
    "# Compute GLCM features for both classes\n",
    "corrosion_features = compute_glcm_features_for_directory(corrosion_dir, \"corrosion\")\n",
    "nocorrosion_features = compute_glcm_features_for_directory(nocorrosion_dir, \"nocorrosion\")\n",
    "\n",
    "# Combine features for both classes\n",
    "all_features = corrosion_features + nocorrosion_features\n",
    "feature_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "feature_df.to_csv(\"./features/glcm_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split(df, column_name, new_column_names):\n",
    "    # Extract the column and remove square brackets\n",
    "    df[column_name] = df[column_name].str.replace('[', '').str.replace(']', '')\n",
    "    \n",
    "    # Split the column values by whitespace and expand them into separate columns\n",
    "    df[new_column_names] = df[column_name].str.split(expand=True)\n",
    "    df.drop(columns=[column_name], inplace=True)\n",
    "\n",
    "\n",
    "df_data = pd.read_csv('./features/glcm_features.csv')\n",
    "\n",
    "\n",
    "columns_to_process = ['Contrast', 'Dissimilarity', 'Homogeneity', 'Energy', 'Correlation', 'ASM']\n",
    "new_column_sets = [['Contrast0', 'Contrast45', 'Contrast90', 'Contrast135'],\n",
    "                   ['Dissimilarity0', 'Dissimilarity45', 'Dissimilarity90', 'Dissimilarity135'],\n",
    "                   ['Homogeneity0', 'Homogeneity45', 'Homogeneity90', 'Homogeneity135'],\n",
    "                   ['Energy0', 'Energy45', 'Energy90', 'Energy135'],\n",
    "                   ['Correlation0', 'Correlation45', 'Correlation90', 'Correlation135'],\n",
    "                   ['ASM0', 'ASM45', 'ASM90', 'ASM135']]\n",
    "\n",
    "# Apply the extraction and splitting function to each set of columns\n",
    "for column, new_columns in zip(columns_to_process, new_column_sets):\n",
    "    extract_and_split(df_data, column, new_columns)\n",
    "\n",
    "\n",
    "new_csv_filename = './features/glcm_features_split.csv'\n",
    "df_data.to_csv(new_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6256983240223464, pca_component = 1\n",
      "Accuracy: 0.6256983240223464, pca_component = 2\n",
      "Accuracy: 0.6787709497206704, pca_component = 3\n",
      "Accuracy: 0.7374301675977654, pca_component = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7402234636871509, pca_component = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770949720670391, pca_component = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7513966480446927, pca_component = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7681564245810056, pca_component = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7737430167597765, pca_component = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7737430167597765, pca_component = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7681564245810056, pca_component = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770949720670391, pca_component = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329, pca_component = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7849162011173184, pca_component = 14\n",
      "Accuracy: 0.7988826815642458, pca_component = 15\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   corrosion       0.81      0.82      0.81       192\n",
      " nocorrosion       0.79      0.77      0.78       166\n",
      "\n",
      "    accuracy                           0.80       358\n",
      "   macro avg       0.80      0.80      0.80       358\n",
      "weighted avg       0.80      0.80      0.80       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./features/glcm_features_split.csv')\n",
    "\n",
    "features = data.drop('Label', axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "n_components = 15\n",
    "\n",
    "for i in range(n_components):\n",
    "\n",
    "    pca = PCA(n_components=i+1)\n",
    "    pca.fit(features_standardized)\n",
    "\n",
    "    # Transform the features\n",
    "    features_pca = pca.transform(features_standardized)\n",
    "\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = data['Label']\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes=(300, ), max_iter=200, random_state=42, verbose=False, solver='adam',alpha=0.0001,\n",
    "                                activation='relu', learning_rate='constant')\n",
    "    mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}, pca_component = {i+1}\")\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/mlp_classifier_model.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained MLP classifier to a file\n",
    "model_filename = './model/mlp_classifier_model.joblib'\n",
    "joblib.dump(mlp_classifier, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 24 features, but MLPClassifier is expecting 15 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m transformed_features_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(transformed_features\u001b[39m.\u001b[39mvalues()))\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Make predictions using the loaded model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m predictions \u001b[39m=\u001b[39m loaded_mlp_classifier\u001b[39m.\u001b[39;49mpredict(transformed_features_array)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m predictions[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHASIL PREDIKSI: Corrosion\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1160\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \n\u001b[1;32m   1149\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    The predicted classes.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(X)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1164\u001b[0m, in \u001b[0;36mMLPClassifier._predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_pass_fast(X, check_input\u001b[39m=\u001b[39;49mcheck_input)\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1167\u001b[0m         y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:207\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict using the trained model\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39mThis is the same as _forward_pass but does not record the activations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m    The decision function of the samples for each class in the model.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m--> 207\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Initialize first layer\u001b[39;00m\n\u001b[1;32m    210\u001b[0m activation \u001b[39m=\u001b[39m X\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 625\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    627\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 24 features, but MLPClassifier is expecting 15 features as input."
     ]
    }
   ],
   "source": [
    "model_filename = './model/mlp_classifier_model.joblib'\n",
    "loaded_mlp_classifier = joblib.load(model_filename)\n",
    "\n",
    "image_path = './resize_data/NOCORROSION/100e35cf19.jpg'\n",
    "target_size = (256, 256)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize Image\n",
    "image_new = ImageOps.fit(image, target_size, method=0, bleed=0.0, centering=(0.5, 0.5))\n",
    "temp_image_path = './temp_resized_image.jpg'\n",
    "image_new.save(temp_image_path)\n",
    "\n",
    "# Extract GLCM Value\n",
    "features = compute_glcm_features(temp_image_path)\n",
    "\n",
    "# Initialize a new dictionary to store the transformed features\n",
    "transformed_features = {}\n",
    "\n",
    "# Define a list of angles\n",
    "angles = ['0', '45', '90', '135']\n",
    "\n",
    "# Iterate through the features and angles to create new labels\n",
    "for feature_name, feature_values in features.items():\n",
    "    for i, angle in enumerate(angles):\n",
    "        new_label = f'{feature_name}{angle}'\n",
    "        transformed_features[new_label] = feature_values[0][i]\n",
    "\n",
    "# Transform the dictionary to a 1D NumPy array\n",
    "transformed_features_array = np.array(list(transformed_features.values())).reshape(1, -1)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_mlp_classifier.predict(transformed_features_array)\n",
    "\n",
    "if predictions[0] == 0:\n",
    "    print(\"HASIL PREDIKSI: Corrosion\")\n",
    "else:\n",
    "    print(\"HASIL PREDIKSI: NoCorrosion\")\n",
    "\n",
    "# Close and remove the temporary image file\n",
    "image_new.close()\n",
    "if os.path.exists(temp_image_path):\n",
    "    os.remove(temp_image_path)\n",
    "\n",
    "image = plt.imread(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Binary Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data success!\n",
      "(1790, 256, 256)\n",
      "(1432, 256, 256)\n",
      "(358, 256, 256)\n",
      "(1432,)\n",
      "(358,)\n"
     ]
    }
   ],
   "source": [
    "def read_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            label = subfolder  # Use the subfolder name as the label\n",
    "            for fn in os.listdir(subfolder_path):\n",
    "                if fn.endswith('.jpg'):\n",
    "                    img_path = os.path.join(subfolder_path, fn)\n",
    "                    im = Image.open(img_path).convert('L')\n",
    "                    data = np.array(im)\n",
    "                    images.append(data)\n",
    "                    labels.append(label)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels from the 'resize_data' folder structure\n",
    "data_folder = './resize_data'\n",
    "images, labels = read_images_from_folder(data_folder)\n",
    "print('Load data success!')\n",
    "\n",
    "X = np.array(images)\n",
    "print(X.shape)\n",
    "\n",
    "# Encode labels \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "radius = 2\n",
    "n_point = radius * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_texture(train_data, test_data):\n",
    "    max_bins_train = 0\n",
    "    max_bins_test = 0\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        lbp = feature.local_binary_pattern(train_data[i], n_point, radius, 'default')\n",
    "        max_bins_train = max(max_bins_train, int(lbp.max()) + 1)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        lbp = feature.local_binary_pattern(test_data[i], n_point, radius, 'default')\n",
    "        max_bins_test = max(max_bins_test, int(lbp.max()) + 1)\n",
    "\n",
    "    train_hist = np.zeros((len(train_data), max_bins_train))\n",
    "    test_hist = np.zeros((len(test_data), max_bins_test))\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        lbp = feature.local_binary_pattern(train_data[i], n_point, radius, 'default')\n",
    "        train_hist[i], _ = np.histogram(lbp, bins=max_bins_train, density=True)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        lbp = feature.local_binary_pattern(test_data[i], n_point, radius, 'default')\n",
    "        test_hist[i], _ = np.histogram(lbp, bins=max_bins_test, density=True)\n",
    "\n",
    "    return train_hist, test_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9636871508379888\n",
      "Test Accuracy: 0.8491620111731844\n",
      "Precision: 0.8502792586951002\n",
      "Recall: 0.8463227911646587\n",
      "F1 Score: 0.8476211495412556\n",
      "Overall Accuracy: 0.8491620111731844\n",
      "Overall Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       192\n",
      "           1       0.86      0.81      0.83       166\n",
      "\n",
      "    accuracy                           0.85       358\n",
      "   macro avg       0.85      0.85      0.85       358\n",
      "weighted avg       0.85      0.85      0.85       358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = lbp_texture(X_train, X_test)\n",
    "\n",
    "# Create and train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the MLP classifier\n",
    "train_accuracy = mlp.score(X_train, y_train)\n",
    "test_accuracy = mlp.score(X_test, y_test)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classify_report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Overall Accuracy: {accuracy}\")\n",
    "print(f\"Overall Accuracy: {classify_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model saved as ./model/mlp_lbp_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the trained MLP model to a file\n",
    "model_filename = './model/mlp_lbp_model.pkl'\n",
    "joblib.dump(mlp, model_filename)\n",
    "\n",
    "print(f\"MLP model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/mlp_lbp_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Call the LBP function on the single image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m lbp_features \u001b[39m=\u001b[39m lbp_texture(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodel/mlp_lbp_model.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m prediction \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict([lbp_features])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/mlp_lbp_model.pkl'"
     ]
    }
   ],
   "source": [
    "def lbp_texture(image):\n",
    "    # Perform LBP feature extraction on a single image\n",
    "    lbp = feature.local_binary_pattern(image, n_point, radius, 'default')\n",
    "    max_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp, bins=max_bins, density=True)\n",
    "    return hist\n",
    "\n",
    "# Load and preprocess a single image\n",
    "image_path = './resize_data/NOCORROSION/01a5aee1ab.jpg'\n",
    "im = Image.open(image_path).convert('L')\n",
    "data = np.array(im)\n",
    "\n",
    "# Define LBP parameters\n",
    "radius = 2\n",
    "n_point = radius * 8\n",
    "\n",
    "# Call the LBP function on the single image\n",
    "lbp_features = lbp_texture(data)\n",
    "\n",
    "loaded_model = joblib.load('model/mlp_lbp_model.pkl')\n",
    "prediction = loaded_model.predict([lbp_features])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gray Level Run Length Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getGrayRumatrix:\n",
    "    data = 0 \n",
    "    def read_img(self,path=\" \"):\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(path) \n",
    "            img = img.convert('L')\n",
    "            self.data=np.array(img)\n",
    "            \n",
    "        except:\n",
    "            img = None\n",
    "            \n",
    "    def getGrayLevelRumatrix(self, array, theta):\n",
    "            '''\n",
    "            \n",
    "            array: the numpy array of the image\n",
    "            theta: Input, the angle used when calculating the gray scale run matrix, list type, can contain fields:['deg0', 'deg45', 'deg90', 'deg135']\n",
    "            glrlm: output,the glrlm result\n",
    "            '''\n",
    "            P = array\n",
    "            x, y = P.shape\n",
    "            min_pixels = np.min(P)   # the min pixel\n",
    "            run_length = max(x, y)   # Maximum parade length in pixels\n",
    "            num_level = np.max(P) - np.min(P) + 1   # Image gray level\n",
    "    \n",
    "            deg0 = [val.tolist() for sublist in np.vsplit(P, x) for val in sublist]   # 0deg\n",
    "            deg90 = [val.tolist() for sublist in np.split(np.transpose(P), y) for val in sublist]   # 90deg\n",
    "            diags = [P[::-1, :].diagonal(i) for i in range(-P.shape[0]+1, P.shape[1])]   #45deg\n",
    "            deg45 = [n.tolist() for n in diags]\n",
    "            Pt = np.rot90(P, 3)   # 135deg\n",
    "            diags = [Pt[::-1, :].diagonal(i) for i in range(-Pt.shape[0]+1, Pt.shape[1])]\n",
    "            deg135 = [n.tolist() for n in diags]\n",
    "    \n",
    "            def length(l):\n",
    "                if hasattr(l, '__len__'):\n",
    "                    return np.size(l)\n",
    "                else:\n",
    "                    i = 0\n",
    "                    for _ in l:\n",
    "                        i += 1\n",
    "                    return i\n",
    "    \n",
    "            glrlm = np.zeros((num_level, run_length, len(theta)))   \n",
    "            for angle in theta:\n",
    "                for splitvec in range(0, len(eval(angle))):\n",
    "                    flattened = eval(angle)[splitvec]\n",
    "                    answer = []\n",
    "                    for key, iter in groupby(flattened):  \n",
    "                        answer.append((key, length(iter)))   \n",
    "                    for ansIndex in range(0, len(answer)):\n",
    "                        glrlm[int(answer[ansIndex][0]-min_pixels), int(answer[ansIndex][1]-1), theta.index(angle)] += 1   \n",
    "            return glrlm\n",
    "            \n",
    "    def apply_over_degree(self,function, x1, x2):\n",
    "        rows, cols, nums = x1.shape\n",
    "        result = np.ndarray((rows, cols, nums))\n",
    "        for i in range(nums):\n",
    "                #print(x1[:, :, i])\n",
    "                result[:, :, i] = function(x1[:, :, i], x2)\n",
    "               # print(result[:, :, i])\n",
    "                result[result == np.inf] = 0\n",
    "                result[np.isnan(result)] = 0\n",
    "        return result \n",
    "    def calcuteIJ (self,rlmatrix):\n",
    "        gray_level, run_length, _ = rlmatrix.shape\n",
    "        I, J = np.ogrid[0:gray_level, 0:run_length]\n",
    "        return I, J+1\n",
    "\n",
    "    def calcuteS(self,rlmatrix):\n",
    "        return np.apply_over_axes(np.sum, rlmatrix, axes=(0, 1))[0, 0]\n",
    "\n",
    "    #1.SRE\n",
    "    def getShortRunEmphasis(self,rlmatrix):\n",
    "            I, J = self.calcuteIJ(rlmatrix)\n",
    "            numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.divide, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
    "            S = self.calcuteS(rlmatrix)\n",
    "            return numerator / S\n",
    "    #2.LRE\n",
    "    def getLongRunEmphasis(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.multiply, rlmatrix, (J*J)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "    #3.GLN\n",
    "    def getGrayLevelNonUniformity(self,rlmatrix):\n",
    "        G = np.apply_over_axes(np.sum, rlmatrix, axes=1)\n",
    "        numerator = np.apply_over_axes(np.sum, (G*G), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "    # 4. RLN\n",
    "    def getRunLengthNonUniformity(self,rlmatrix):\n",
    "            R = np.apply_over_axes(np.sum, rlmatrix, axes=0)\n",
    "            numerator = np.apply_over_axes(np.sum, (R*R), axes=(0, 1))[0, 0]\n",
    "            S = self.calcuteS(rlmatrix)\n",
    "            return numerator / S\n",
    "\n",
    "        # 5. RP\n",
    "    def getRunPercentage(self,rlmatrix):\n",
    "            gray_level, run_length,_ = rlmatrix.shape\n",
    "            num_voxels = gray_level * run_length\n",
    "            return self.calcuteS(rlmatrix) / num_voxels\n",
    "\n",
    "        # 6. LGLRE\n",
    "    def getLowGrayLevelRunEmphasis(self,rlmatrix):\n",
    "            I, J = self.calcuteIJ(rlmatrix)\n",
    "            numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.divide, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
    "            S = self.calcuteS(rlmatrix)\n",
    "            return numerator / S\n",
    "\n",
    "        # 7. HGL   \n",
    "    def getHighGrayLevelRunEmphais(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.multiply, rlmatrix, (I*I)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "\n",
    "        # 8. SRLGLE\n",
    "    def getShortRunLowGrayLevelEmphasis(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.divide, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    "    # 9. SRHGLE\n",
    "    def getShortRunHighGrayLevelEmphasis(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        temp = self.apply_over_degree(np.multiply, rlmatrix, (I*I))\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    " \n",
    "    # 10. LRLGLE\n",
    "    def getLongRunLow(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        temp = self.apply_over_degree(np.multiply, rlmatrix, (J*J))\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.divide, temp, (J*J)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S\n",
    " \n",
    "    # 11. LRHGLE\n",
    "    def getLongRunHighGrayLevelEmphais(self,rlmatrix):\n",
    "        I, J = self.calcuteIJ(rlmatrix)\n",
    "        numerator = np.apply_over_axes(np.sum, self.apply_over_degree(np.multiply, rlmatrix, (I*I*J*J)), axes=(0, 1))[0, 0]\n",
    "        S = self.calcuteS(rlmatrix)\n",
    "        return numerator / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "glr_matrix_calculator = getGrayRumatrix()\n",
    "glr_matrix_calculator.read_img(\"./resize_data/CORROSION/000001.jpg\")\n",
    "\n",
    "theta = ['deg0', 'deg45', 'deg90', 'deg135']\n",
    "glrlm_result = glr_matrix_calculator.getGrayLevelRumatrix(glr_matrix_calculator.data, theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84488/699285923.py:63: RuntimeWarning: divide by zero encountered in divide\n",
      "  result[:, :, i] = function(x1[:, :, i], x2)\n",
      "/tmp/ipykernel_84488/699285923.py:63: RuntimeWarning: invalid value encountered in divide\n",
      "  result[:, :, i] = function(x1[:, :, i], x2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "root_dir = \"./resize_data\"\n",
    "subdirectories = [\"CORROSION\", \"NOCORROSION\"]\n",
    "\n",
    "header = [\"Label\", \"SRE\", \"LRE\", \"GLN\", \"RLN\", \"RP\", \"LGLRE\", \"HGL\", \"SRLGLE\", \"SRHGLE\", \"LRLGLE\", \"LRHGLE\"]\n",
    "\n",
    "with open(\"./features/glrlm_image_features.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "for label, subdirectory in enumerate(subdirectories):\n",
    "    subdir_path = os.path.join(root_dir, subdirectory)\n",
    "\n",
    "    for filename in os.listdir(subdir_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(subdir_path, filename)\n",
    "\n",
    "            # Read image and calculate GLRLM\n",
    "            glr_matrix_calculator.read_img(image_path)\n",
    "            theta = ['deg0', 'deg45', 'deg90', 'deg135']\n",
    "            glrlm_result = glr_matrix_calculator.getGrayLevelRumatrix(glr_matrix_calculator.data, theta)\n",
    "\n",
    "            # Calculate features\n",
    "            sre_result = glr_matrix_calculator.getShortRunEmphasis(glrlm_result)\n",
    "            lre_result = glr_matrix_calculator.getLongRunEmphasis(glrlm_result)\n",
    "            gln_result = glr_matrix_calculator.getGrayLevelNonUniformity(glrlm_result)\n",
    "            rln_result = glr_matrix_calculator.getRunLengthNonUniformity(glrlm_result)\n",
    "            rp_result = glr_matrix_calculator.getRunPercentage(glrlm_result)\n",
    "            lglre_result = glr_matrix_calculator.getLowGrayLevelRunEmphasis(glrlm_result)\n",
    "            hgl_result = glr_matrix_calculator.getHighGrayLevelRunEmphais(glrlm_result)\n",
    "            srlgle_result = glr_matrix_calculator.getShortRunLowGrayLevelEmphasis(glrlm_result)\n",
    "            srhgle_result = glr_matrix_calculator.getShortRunHighGrayLevelEmphasis(glrlm_result)\n",
    "            lrlgle_result = glr_matrix_calculator.getLongRunLow(glrlm_result)\n",
    "            lrhgle_result = glr_matrix_calculator.getLongRunHighGrayLevelEmphais(glrlm_result)\n",
    "\n",
    "            # Append results to CSV file\n",
    "            with open(\"./features/glrlm_image_features.csv\", mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([label, sre_result, lre_result, gln_result, rln_result, rp_result, lglre_result,\n",
    "                                hgl_result, srlgle_result, srhgle_result, lrlgle_result, lrhgle_result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cek = pd.read_csv('./features/glrlm_image_features.csv')\n",
    "\n",
    "# Extract the 'Contrast' column and remove square brackets\n",
    "df_cek['SRE'] = df_cek['SRE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['LRE'] = df_cek['LRE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['GLN'] = df_cek['GLN'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['RLN'] = df_cek['RLN'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['RP'] = df_cek['RP'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['LGLRE'] = df_cek['LGLRE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['HGL'] = df_cek['HGL'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['SRLGLE'] = df_cek['SRLGLE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['SRHGLE'] = df_cek['SRHGLE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['LRLGLE'] = df_cek['LRLGLE'].str.replace('[', '').str.replace(']', '')\n",
    "df_cek['LRHGLE'] = df_cek['LRHGLE'].str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "\n",
    "# Split the 'SRE' column values by whitespace and expand them into separate columns\n",
    "df_cek[['SRE0', 'SRE45', 'SRE90', 'SRE135']] = df_cek['SRE'].str.split(expand=True)\n",
    "df_cek[['LRE0', 'LRE45', 'LRE90', 'LRE135']] = df_cek['LRE'].str.split(expand=True)\n",
    "df_cek[['GLN0', 'GLN45', 'GLN90', 'GLN135']] = df_cek['GLN'].str.split(expand=True)\n",
    "df_cek[['RLN0', 'RLN45', 'RLN90', 'RLN135']] = df_cek['RLN'].str.split(expand=True)\n",
    "df_cek[['RP0', 'RP45', 'RP90', 'RP135']] = df_cek['RP'].str.split(expand=True)\n",
    "df_cek[['LGLRE0', 'LGLRE45', 'LGLRE90', 'LGLRE135']] = df_cek['LGLRE'].str.split(expand=True)\n",
    "df_cek[['HGL0', 'HGL45', 'HGL90', 'HGL135']] = df_cek['HGL'].str.split(expand=True)\n",
    "df_cek[['SRLGLE0', 'SRLGLE45', 'SRLGLE90', 'SRLGLE135']] = df_cek['SRLGLE'].str.split(expand=True)\n",
    "df_cek[['SRHGLE0', 'SRHGLE45', 'SRHGLE90', 'SRHGLE135']] = df_cek['SRHGLE'].str.split(expand=True)\n",
    "df_cek[['LRLGLE0', 'LRLGLE45', 'LRLGLE90', 'LRLGLE135']] = df_cek['LRLGLE'].str.split(expand=True)\n",
    "df_cek[['LRHGLE0', 'LRHGLE45', 'LRHGLE90', 'LRHGLE135']] = df_cek['LRHGLE'].str.split(expand=True)\n",
    "\n",
    "\n",
    "\n",
    "# Drop the original 'Contrast' column\n",
    "df_cek.drop(columns=['SRE'], inplace=True)\n",
    "df_cek.drop(columns=['LRE'], inplace=True)\n",
    "df_cek.drop(columns=['GLN'], inplace=True)\n",
    "df_cek.drop(columns=['RLN'], inplace=True)\n",
    "df_cek.drop(columns=['RP'], inplace=True)\n",
    "df_cek.drop(columns=['LGLRE'], inplace=True)\n",
    "df_cek.drop(columns=['HGL'], inplace=True)\n",
    "df_cek.drop(columns=['SRLGLE'], inplace=True)\n",
    "df_cek.drop(columns=['SRHGLE'], inplace=True)\n",
    "df_cek.drop(columns=['LRLGLE'], inplace=True)\n",
    "df_cek.drop(columns=['LRHGLE'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a new CSV file\n",
    "new_csv_filename = './features/glrlm_features_split.csv'\n",
    "df_cek.to_csv(new_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6983240223463687\n",
      "n_composition =  1\n",
      "Accuracy: 0.6899441340782123\n",
      "n_composition =  2\n",
      "Accuracy: 0.7094972067039106\n",
      "n_composition =  3\n",
      "Accuracy: 0.7011173184357542\n",
      "n_composition =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.723463687150838\n",
      "n_composition =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7150837988826816\n",
      "n_composition =  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7067039106145251\n",
      "n_composition =  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7122905027932961\n",
      "n_composition =  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7206703910614525\n",
      "n_composition =  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7458100558659218\n",
      "n_composition =  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.729050279329609\n",
      "n_composition =  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7374301675977654\n",
      "n_composition =  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7402234636871509\n",
      "n_composition =  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7430167597765364\n",
      "n_composition =  14\n",
      "Accuracy: 0.7430167597765364\n",
      "n_composition =  15\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77       192\n",
      "           1       0.75      0.67      0.71       166\n",
      "\n",
      "    accuracy                           0.74       358\n",
      "   macro avg       0.74      0.74      0.74       358\n",
      "weighted avg       0.74      0.74      0.74       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./features/glrlm_features_split.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(X)\n",
    "n_components = 15\n",
    "\n",
    "for i in range(n_components):\n",
    "    pca = PCA(n_components=i+1)\n",
    "    pca.fit(features_standardized)\n",
    "\n",
    "    # Transform the features\n",
    "    features_pca = pca.transform(features_standardized)\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    glrlm_mlp_classifier = MLPClassifier(hidden_layer_sizes=(200, ), max_iter=200, random_state=42, activation='relu', verbose=False, solver='adam',\n",
    "                                         alpha=0.0001 )\n",
    "    glrlm_mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = glrlm_mlp_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print('n_composition = ',i+1)\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/glrlm_classifier_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained MLP classifier to a file\n",
    "model_filename = './model/glrlm_classifier_model.joblib'\n",
    "joblib.dump(glrlm_mlp_classifier, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
